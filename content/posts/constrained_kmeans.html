---
title: "Constrained k-means clustering for stratification"
author: "Tom Brailey"
date: "2020-09-29"
categories: ["R"]
tags: ["R", "kmeans", "hierarchical-clustering"]
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p>I’ve been learning a lot about randomization over the past few weeks. Specifically, I have been reading up on the benefits of matched-pair designs over basic randomization. Bai (2020a, 2020b), Athey and Imbens (2016), and Imai et al. (2020), each provide theoretical and practical insights into the benefits of matched-pair randomization, where, in a simple treatment-control design, the researcher creates pairs of units at the level of randomization based on some baseline characteristic (e.g. income, population, geographic size, proximity, etc.), and then randomly assigns one of the pairs into the treatment, and the other into the control.</p>
<p>My curiosity was piqued when I tried to find papers that used matched-pairs for a multi-arm experiment, that is, an experiment with more than one treatment (or control) group. Lu et al. (2011), Lu and Rosenbaum (2004), and Greevy (2004) talk about optimal nonbipartite matching (which sounds very fancy) but only consider instances with three treatment arms. Having just worked on a project with five arms, I was interested to see if any work focused on quinquepartite matching, and quickly realized that no such work existed. Realizing also that quinquepartite matching was a ridiculous prospect, I think set about thinking of ways to do something similar. I had in mind a sort of clustering and stratification method that grouped similar units together, and then randomly assigned the different treatment groups within those clusters.</p>
<p>And so began my adventure into constrained k-means clustering…</p>
<p>Having conducted a brief review on clustering and stratification methods for randomization, I realized that what I was looking for was a function that grouped similar units together. K-means seemed like a solid candidate to do so. However, k-means clustering algorithms don’t like to have a size constraint on them. They prefer to split the data into groups based on characteristics, not based on quotas. This was my first challenge, figuring out whether it was statistically legal to put a cluster size constraint on a k-means algorithm and whether I could figure out a way to do this in R.</p>
<p>The second challenge arose during a discussion with the PIs at PGRP. Say we were creating groups of units based on geographical proximity and population size. We would be matching on three variables, the latitude, longitude, and population number. If these covariates were all equally weighted, we wouldn’t reap the logistical benefits of keeping the units close together, as the population variable might cause the algorithm to match units that end up being pretty far away. We also have reason to believe that geographically proximate units are more likely to be similar on other baseline characteristics. The problem then becomes coming up with a good way to weight the covariates on which we are clustering. This problem will be addressed as a separate article at a later date…</p>
<p>After a good deal of having no idea of what was going on, several discussions with my co-workers, and literally days of stackoverflow digging, I arrived at a solution.</p>
<p>First, let’s get set up:</p>
<pre class="r"><code>library(magrittr)
library(ggplot2)</code></pre>
<p>Now, let’s generate some datums:</p>
<pre class="r"><code># Set seed for reproducibility
set.seed(1234)

dat &lt;-
  expand.grid(
    district         = c(1:2),
    sub_district     = c(1:7),
    sub_sub_district = c(1:19), 
    village_id       = c(1:2) 
  ) %&gt;% 
  dplyr::group_by(district, sub_district, sub_sub_district, village_id) %&gt;%
  dplyr::mutate(
    # Total population
    tot_pop = rnorm(n = 1, mean = 100, sd = 5000),
    # Number of primary schools
    p_schl = rnorm(n = 1, mean = 2, sd = 6),
    # Paved road
    p_road = sample(0:1, size = dplyr::row_number(), replace = FALSE)
  ) %&gt;% 
  dplyr::group_by(district, sub_district, sub_sub_district, village_id) %&gt;% 
  dplyr::mutate(
    # Size of village in hectares
    town_hec = rnorm(n = 1, mean = 300, sd = 320)
  ) %&gt;% 
  dplyr::group_by(district, sub_district, sub_sub_district, village_id) %&gt;%
  dplyr::mutate(
    # Coordinates
    x_cent = rnorm(n = 1, mean = 99.9, sd = 0.66), 
    y_cent = rnorm(n = 1, mean = 33.3, sd = 0.33)
  ) %&gt;% 
  dplyr::ungroup()</code></pre>
<p>Let’s look at dat:</p>
<pre class="r"><code>dat</code></pre>
<pre><code>## # A tibble: 532 x 10
##    district sub_district sub_sub_district village_id tot_pop p_schl p_road
##       &lt;int&gt;        &lt;int&gt;            &lt;int&gt;      &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;
##  1        1            1                1          1  -5935.  0.361      1
##  2        2            1                1          1  -1093. -5.50       1
##  3        1            2                1          1  -1371.  0.663      0
##  4        2            2                1          1    218. 15.5        1
##  5        1            3                1          1  -6854. -4.88       1
##  6        2            3                1          1   4877. -7.11       0
##  7        1            4                1          1    212.  1.31       1
##  8        2            4                1          1  -9075.  6.04       0
##  9        1            5                1          1   8294. -4.69       1
## 10        2            5                1          1   2888. -2.48       1
## # ... with 522 more rows, and 3 more variables: town_hec &lt;dbl&gt;, x_cent &lt;dbl&gt;,
## #   y_cent &lt;dbl&gt;</code></pre>
<p>dat looks good.</p>
<p>Now, above I mentioned that the whole “figuring out a way to apply econometrically sensible/not totally arbitrary weights to our covariates” conundrum would be tackled in a later article, so for the time being, we can be very arbitrary in how we apply weights. In any case, we want to apply a larger weight to the spatial variables as we believe that spatial covariates are more important than aspatial ones when it comes to clustering. Below, we see that the latitude and longitude are weighted 10:1 with the population variable.</p>
<pre class="r"><code>dat &lt;- 
  dplyr::mutate(
    dat,
    x_cent = scales::rescale(x_cent, to = c(0, 10)),
    y_cent = scales::rescale(y_cent, to = c(0, 10)),
    tot_pop = scales::rescale(tot_pop, to = c(0, 1))
    )</code></pre>
<p>Hierarchical clustering—the fancy word for what we are doing right now—algorithms require a distance matrix to generate the right clusters. That means we need to subset our entire dataset to only the variables that we want to compute distances for, i.e., the three variables above.</p>
<pre class="r"><code># Subset
dat &lt;- dplyr::select(dat, x_cent, y_cent, tot_pop)

# Calculate distances
dist &lt;- distances::distances(as.data.frame(dat))</code></pre>
<p>Now comes the cool bit. I think this next segment is made even cooler because I don’t think there’s ever been a time when I’ve had an issue, searched for the solution, and found a package that does <em>exactly</em> what I wanted it to do in literally one line of code. The whole experience felt too good to be true (maybe it is – please email me if you see me making any statistically illegal claims. Thanks.).</p>
<p>The package <code>scclust</code>—size-constrained clustering—does exactly what it says on the CRAN tin. In baby terms, it just takes a k-means algorithm and puts a size constraint on it. Given that I have been talking about studies with five treatment arms, I put a size constraint of 5 below.</p>
<pre class="r"><code>clust &lt;- scclust::hierarchical_clustering(distances = dist, size_constraint = 5)</code></pre>
<p>Magical. Now let’s clean up our datasets by joining <code>clust</code> with <code>dat</code>.</p>
<pre class="r"><code>final &lt;- 
  dplyr::bind_cols(dat, clust) %&gt;% 
  dplyr::rename(block = `...4`)</code></pre>
<pre><code>## New names:
## * NA -&gt; ...4</code></pre>
<p>Let’s have a quick peek at the output to make sure it’s doing what we hope it’s doing:</p>
<pre class="r"><code>investigate_cluster &lt;- 
  dplyr::group_by(final, block) %&gt;% 
  dplyr::summarise(count = length(block))</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre class="r"><code>investigate_cluster</code></pre>
<pre><code>## # A tibble: 106 x 2
##    block     count
##    &lt;scclust&gt; &lt;int&gt;
##  1 0             5
##  2 1             5
##  3 2             5
##  4 3             5
##  5 4             5
##  6 5             5
##  7 6             5
##  8 7             5
##  9 8             5
## 10 9             5
## # ... with 96 more rows</code></pre>
<p>Looks good. It looks like each block is of size 5. Let’s double check:</p>
<pre class="r"><code>max(investigate_cluster$count)</code></pre>
<pre><code>## [1] 7</code></pre>
<pre class="r"><code>table(investigate_cluster$count)</code></pre>
<pre><code>## 
##   5   7 
## 105   1</code></pre>
<p>Interesting, looks like they’re not equal. So, because <code>dat</code> contains 532 observations (which, if my limited mathematical knowledge pays off, is not divisible by five), the algorithm automatically creates a sort of “overflow” group. Neat. This isn’t super convenient in my case, so I will add an extra section on how to alter this default.</p>
<p>Anyway, let’s now visualize the algorithm on our dummy data. I use the nifty package <code>ggConvexHull</code> to visualize the clusters.</p>
<pre class="r"><code>ggplot(
  final, 
  mapping = aes(
    x = x_cent, 
    y = y_cent, 
    color = factor(block)
    )
  ) +
  geom_point() +
  ggConvexHull::geom_convexhull(
    alpha = .5, 
    aes(
      fill = factor(block)
      )
    ) +
  theme_bw() + 
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/posts/constrained_kmeans_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>We can see that the points have been grouped in fives, except for that one cluster in the bottom right-hand corner. Notice how that cluster of seven has another cluster protruding into it – had we not included <code>tot_pop</code> in the distance matrix, each cluster would be entirely spatially distinct as we would clustering solely based on location.</p>
<p>So that is how we can create clusters based on spatial and aspatial covariates.</p>
<div id="extra-stuff" class="section level3">
<h3>Extra stuff</h3>
<p>I don’t like that pesky group of seven. I would rather create an overflow group of two, and keep those as spare units in case we need them. I will run through the procedure in-line below:</p>
<pre class="r"><code># Define number of treatment waves
number_of_waves &lt;- 5

# REPRODUCIBILITY!!!
set.seed(1234)

final_treated &lt;-
  final %&gt;%
  dplyr::arrange(block) %&gt;% 
  dplyr::group_by(block) %&gt;% 
  
  # 1. Find the number of observations in each block 
  dplyr::mutate(block_count = dplyr::row_number()) %&gt;% 
  dplyr::ungroup() %&gt;% 
  
  # 2. Create overflow group by setting block cutoff == number_of_waves
  dplyr::mutate(
    block = ifelse(
      block_count &gt; number_of_waves, 
      max(block) + 1, 
      block
    )
  ) %&gt;% 
  
  # 3. Re-calculate block_count
  dplyr::group_by(block) %&gt;% 
  dplyr::mutate(block_count = max(dplyr::row_number())) %&gt;% 
  
  # 4. Apply treatment status
  dplyr::mutate(
    treatment = ifelse(
      block_count == number_of_waves, 
      sample(c(number_of_waves), dplyr::n(), replace = FALSE), 
      sample(sample(number_of_waves, max(block_count)), dplyr::n(), replace = FALSE)
    )
  )</code></pre>
<p>Point 4. needs some extra explanation. For clusters that have exactly five units with them are randomly assigned one of the five treatment waves or phases. Then, for clusters that do not contain the exact number of waves—e.g. that cluster of two observations—we randomly assign them to any of the five waves.</p>
<p>Let’s visualize it.</p>
<pre class="r"><code># Plot results
treated_visual &lt;- 
  ggplot() +
  geom_point(
    final_treated, 
    mapping = aes(
      x = x_cent, 
      y = y_cent, 
      color = factor(treatment)
      ), 
    size = 1
    ) +
  ggConvexHull::geom_convexhull(
    final_treated, 
    alpha = .5, 
    mapping = aes(
      x = x_cent, 
      y = y_cent, 
      fill = factor(block)
      )
    ) +
  scale_fill_manual(
    values = rep(&quot;grey&quot;, length(unique(final_treated$block)))
    ) +
  scale_color_manual(
    values = c(&quot;1&quot; = &quot;dodgerblue4&quot;, &quot;2&quot; = &quot;turquoise4&quot;, &quot;3&quot; = &quot;red4&quot;, &quot;4&quot; = &quot;orange2&quot;, &quot;5&quot; = &quot;yellow3&quot;)
    ) + 
  labs(
    x = &quot;&quot;, 
    y = &quot;&quot;, 
    color = &quot;Treatment group&quot;
    ) +
  guides(fill = FALSE) + 
  theme_bw()

treated_visual</code></pre>
<p><img src="/posts/constrained_kmeans_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>That concludes today’s post. This will lead us nicely into the next ridiculously niche plot post, which will give an overview of calculating optimal euclidean distances between units within a cluster. Stay tuned!</p>
</div>
