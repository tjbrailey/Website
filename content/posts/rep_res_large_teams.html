---
title: "Reproducible research in large and complex teams"
author: "Tom Brailey"
date: "2021-12-03"
categories: ["R"]
tags: ["R", "replication"]
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>This month, I was awarded a grant from the <a href="https://www.bitss.org/">Berkeley Institute for Transparency in the Social Sciences (BITSS)</a> to develop and deliver training in reproducible research practices. This grant enabled me to create resources designed to help new hires at our organization become familiar with GitHub, GitHub Desktop, version control, and reproducible coding practices in R. As part of the grant, I wrote a short piece about the barriers to ensuring reproducibility within large teams and organizations embedded within non-academic structures. The article can be read on the BITSS blog page, but I am including it here for completeness.</p>
<div id="ensuring-reproducibility-in-large-research-teams" class="section level1">
<h1>Ensuring reproducibility in large research teams</h1>
<p>Holding all else equal, ensuring a reproducible and transparent research pipeline is more straightforward with fewer team members. When we discuss achieving reproducible social science in the abstract, there are four broad steps that need clear documentation: 1) obtaining the data; 2) cleaning and wrangling the data; 3) analyzing and visualizing the data; and 4) releasing the data to the public.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> With a few principle investigators and research assistants to collect and work on the data, this process has been, in my experience, relatively straightforward. However, ensuring a reproducible workflow becomes markedly more tricky when the project has many team members or is integrated into non-academic bodies such as non-profits or governments.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Such organizations face an uphill battle in keeping to the ground rules of transparent and ethical research, especially if their partners do not place emphasis on the norms of transparent social science.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>One might assume that whatever works for a small research team simply scales up for larger teams but I would argue that far more care needs to be taken with the latter. This is because individual team members will have different levels of exposure to reproducible practices, expectations of the research process, and deliverables and responsibilities. Does non-analysis code (e.g. back-checks, logic checks, cleaning and recoding code) need to be treated the same as analysis code, even though it won’t get included in a manuscript’s replication package? Do policy reports or updates for government officials need to emphasize replicability, even if those industries are not placing the same emphasis on transparency as in academia? The answers to these questions, I believe, are absolutely, yes. With that said, there appears to be very little literature focusing on this particular aspect of reproducible social science, so I will discuss some concrete options to ensure transparency in large research teams.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>First, it is important to ensure that all code, irrespective of what it does or who it is for, is version controlled. The industry standard (at least in political science) version control software is GitHub, and there are plenty of useful guides for getting this set up. Broadly speaking, each project should be stored as a single repository, with separate folders for cleaning, analysis, and replication code. Each researcher should create their own pull request when working on a specific task, then assign another RA to review the changes before merging them into the main branch. Beyond reproducibility, this method ensures accountability among researchers, and allows teams to see all changes made to code files from the beginning of time.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> Datasets can be stored on GitHub but it is not necessary to do this, given that there usually isn’t a reason to overwrite a raw dataset. Similarly, documents (.word, .tex, .pdf, etc.) can be stored on GitHub and version controlled, but it is not considered industry standard to do so. A bifurcated system where all code is version controlled, and non-code files are kept in a shared storage space can work well for large research teams, though for simplicity, storing all files on GitHub (e.g. linked through the repositories Wiki page) might be advantageous.</p>
<p>Second, within this reproducible framework, it is important to ensure that cleaning and analysis is kept parsimonious and well documented. The findings that you publish and present to governments may well be replicable, but if it is based on bad analysis, then it is meaningless. A 2015 PNAS article suggests that the best way to prevent replicable, but poor, analysis is to “increase the number of trained data analysts in the scientific community and […] identify statistical software and tools that can be shown to improve reproducibility and replicability of studies”.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> Having a well documented and organization-wide standard for conducting data analysis and data visualization helps to thwart potential mistakes and misleading results.</p>
<p>Third, large research teams should encourage non-academic entities with whom they interact to publish codebooks and thorough documentation with any data that they share. Even if this data is not to be shared with the broader public, it is important for the research team to know exactly how the data were generated. It is exciting to see organizations such as JPAL focus on bridging the gap between their survey experiments and the administrative data they use for analysis. JPAL’s Innovations in Data and Experiments for Action Initiative “supports governments, firms, and non-profit organizations […] who want to make their administrative data accessible in a safe and ethical way”.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> With a survey, the research team has full control over the instrument and knows exactly how each variable is generated, but it is just as important to verify the validity of any external data used for analysis because bad data, like the bad analysis practices discussed above, causes misleading results.</p>
<p>Fourth, it is useful to have at least one team member, or an outside consultant, who remains up-to-date on the latest reproducible science practices, who is able to monitor the codebase, and who is able to offer training to the team members. This ensures that all researchers working with code and data are able to collaborate in a single repository with ease. It is important that all team members, even those who are not in direct contact with code and data, are aware of the importance of the reproducible best practices and have exposure to the version control software that their team uses.</p>
<p>In this article, I have outlined some of the challenges faced by large research teams with regards to ensuring transparency throughout their research pipeline. I have also pointed to a few potentially useful practices that can help these diverse and complex organizations adhere to the tenets of reproducible social science. It would be great to hear from other researchers about their experiences in large research teams and collate resources that will benefit similar organizations in the future.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>These four steps are an abridged version of Munafò MR, Nosek BA, Bishop DVM, Button KS, Chambers CD, et al. 2017. A manifesto for reproducible science. Nat Hum Behav. 1(1):0021.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>For the purposes of this article, I will refer to “large research teams” and research projects that either involve a large number of team members or those which are embedded in non-academic settings.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>See <a href="https://www.bitss.org/working-toward-a-common-rule-for-transparent-reproducible-and-ethical-research/">this BITSS article</a> for a more thorough discussion of the tenets of transparent social science and the Mertonian norms.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="https://dimewiki.worldbank.org/Reproducible_Research">This World Bank guide</a> offers a fantastic overview of the whole research pipeline for large teams, but does not focus on the interplay between, and challenges faced by, the whole team.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Dropbox, for example, only allows version history tracking for 180 days.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Leek JT, Peng RD. 2015. Opinion: Reproducible research can still be wrong: Adopting a prevention approach. Proc Natl Acad Sci USA. 112(6):1645–46.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>See <a href="https://www.povertyactionlab.org/initiative/innovations-data-and-experiments-action-initiative">IDEA’s website</a> for more information on who they are and what they do.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
